import torch
import torch.nn as nn
import cv2
import numpy as np
from PIL import Image
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
from transformers import pipeline
import clip  # pip install git+https://github.com/openai/CLIP.git

class CLIPResNet50Regressor(nn.Module):
    def __init__(self, device='cuda'):
        super(CLIPResNet50Regressor, self).__init__()
        # CLIP ë¡œë“œ (JIT=False í•„ìˆ˜)
        self.clip_model, _ = clip.load("RN50", device=device, jit=False)
        self.visual_encoder = self.clip_model.visual.float()
        
        visual_dim = 1024 
        intensity_dim = 1
        
        self.regressor = nn.Sequential(
            nn.Linear(visual_dim + intensity_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, images, intensities):
        visual_features = self.visual_encoder(images.float())
        combined = torch.cat((visual_features, intensities), dim=1)
        output = self.regressor(combined)
        return output

class ModelWrapper(torch.nn.Module):
    def __init__(self, model):
        super(ModelWrapper, self).__init__()
        self.model = model
        self.current_intensity = None 
    
    def forward(self, x):
        return self.model(x, self.current_intensity)

class MultimodalPredictor:
    def __init__(self, model_path):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Loading CLIP-ResNet50 Model on {self.device}...")

        _, self.clip_preprocess = clip.load("RN50", device=self.device, jit=False)
        
        original_model = CLIPResNet50Regressor(device=self.device).to(self.device)
        checkpoint = torch.load(model_path, map_location=self.device)
        original_model.load_state_dict(checkpoint)
        original_model.eval()

        self.wrapper = ModelWrapper(original_model).to(self.device)
        
        self.target_layers = [self.wrapper.model.visual_encoder.layer4[-1]]
        self.cam = GradCAM(model=self.wrapper, target_layers=self.target_layers)
        
        print("Loading Sentiment Analysis Pipeline...")
        self.sentiment_analyzer = pipeline("zero-shot-image-classification", 
                                           model="openai/clip-vit-base-patch32", 
                                           device=0 if torch.cuda.is_available() else -1)

    def get_intensity(self, image_pil):
        candidate_labels = ["neutral", "positive", "negative"]
        try:
            results = self.sentiment_analyzer(image_pil, candidate_labels=candidate_labels)
            scores = {res['label']: res['score'] for res in results}
            intensity = 1.0 - scores.get('neutral', 0.0)
        except Exception:
            intensity = 0.5 
        return intensity

    def predict(self, image_path, save_heatmap=True):
        img_pil = Image.open(image_path).convert('RGB')
        img_np = np.array(img_pil.resize((224, 224))) / 255.0
        
        img_tensor = self.clip_preprocess(img_pil).unsqueeze(0).to(self.device)
        
        intensity_val = self.get_intensity(img_pil)
        intensity_tensor = torch.tensor([[intensity_val]], dtype=torch.float32).to(self.device)
        
        print(f"ğŸ“Š ê°ì • ê°•ë„(Intensity): {intensity_val:.4f}")
        self.wrapper.current_intensity = intensity_tensor

        with torch.no_grad():
            score = self.wrapper(img_tensor).item()

        targets = [ClassifierOutputTarget(0)]
        
        try:
            grayscale_cam = self.cam(input_tensor=img_tensor, targets=targets)[0, :]
            visualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)
            
            if save_heatmap:
                final_score = score * 100
                save_name = f"clip_result_{final_score:.2f}_inten{intensity_val:.2f}.jpg"
                cv2.imwrite(save_name, cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))
                print(f"ğŸ“¸ ê²°ê³¼ ì €ì¥: {save_name}")
                
        except Exception as e:
            print(f"âš ï¸ Grad-CAM ìƒì„± ì‹¤íŒ¨: {e}")
            visualization = None

        return score * 100, intensity_val

if __name__ == "__main__":

    MODEL_PATH = ""
    
    # [ì„¤ì •] í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ
    TEST_IMAGE = ""
    
    # ì‹¤í–‰
    import os
    if os.path.exists(MODEL_PATH) and os.path.exists(TEST_IMAGE):
        predictor = MultimodalPredictor(MODEL_PATH)
        final_score, intensity = predictor.predict(TEST_IMAGE)
        print("-" * 30)
        print(f"ğŸ¯ CLIP-AI ì˜ˆì¸¡ ì ìˆ˜: {final_score:.2f}ì ")
        print("-" * 30)
    else:
        print("âŒ ê²½ë¡œ ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ì´ë‚˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ëª¨ë¸: {MODEL_PATH}")
        print(f"ì´ë¯¸ì§€: {TEST_IMAGE}")